{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplex Network Construction Documentation\n",
    "\n",
    "In this document, we describe the construction of a multiplex network based on the incident data from the Oklahoma Gas and Electric company. The multiplex network consists of multiple layers, each representing different types of connections between the substations.\n",
    "\n",
    "#### Layers in the Multiplex Network\n",
    "\n",
    "1. **Job Region**\n",
    "   - **Description**: This layer represents the geographical regions where the substations are located. Nodes (substations) are connected if they belong to the same job region.\n",
    "   \n",
    "2. **Job Area (DISTRICT)**\n",
    "   - **Description**: This layer represents the specific districts within the regions. Nodes are connected if they belong to the same job area or district.\n",
    "   \n",
    "3. **Month/Day/Year**\n",
    "   - **Description**: This temporal layer represents the date on which incidents occurred. Nodes are connected if incidents at these substations occurred on the same day.\n",
    "   \n",
    "4. **Custs Affected Interval**\n",
    "   - **Description**: This layer categorizes incidents based on the number of customers affected. Nodes are connected if the number of affected customers falls within the same interval (Very Low, Low, Medium, High).\n",
    "   \n",
    "5. **OGE Causes**\n",
    "   - **Description**: This layer categorizes incidents based on their causes as defined by the Oklahoma Gas and Electric company. Nodes are connected if incidents share the same cause.\n",
    "   \n",
    "6. **Major Storm Event (Yes or No)**\n",
    "   - **Description**: This layer represents whether an incident occurred during a major storm event. Nodes are connected if incidents at these substations were affected by the same storm event (Yes or No).\n",
    "   \n",
    "7. **Distribution, Substation, Transmission Type**\n",
    "   - **Description**: This layer represents the type of infrastructure associated with the incidents. Nodes are connected if they belong to the same type, such as distribution, substation, or transmission.\n",
    "\n",
    "These layers collectively provide a comprehensive view of the different relationships and interactions between the substations based on various criteria, enabling a detailed analysis of the incident data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell 01 # Multiplex Network Creation from the selected features\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Preprocess data: replace spaces in 'Job Substation' names with underscores\n",
    "data['Job Substation'] = data['Job Substation'].str.replace(' ', '_')\n",
    "\n",
    "# Define intervals for 'Custs Affected Interval'\n",
    "custs_intervals = {\n",
    "    'Very Low': (0, 50),\n",
    "    'Low': (51, 100),\n",
    "    'Medium': (101, 500),\n",
    "    'High': (501, float('inf'))\n",
    "}\n",
    "\n",
    "def categorize_custs_affected(affected):\n",
    "    for category, (low, high) in custs_intervals.items():\n",
    "        if low <= affected <= high:\n",
    "            return category\n",
    "    return 'Unknown'\n",
    "\n",
    "# Add a column for categorized customer affected intervals\n",
    "data['Custs Affected Interval'] = data['Custs Affected'].apply(categorize_custs_affected)\n",
    "\n",
    "# Initialize a dictionary to hold each layer's graph\n",
    "layers = {\n",
    "    'Job Region': nx.Graph(),\n",
    "    'Job Area (DISTRICT)': nx.Graph(),\n",
    "    'Time': nx.Graph(),\n",
    "    'Custs Affected Interval': nx.Graph(),\n",
    "    'OGE Causes': nx.Graph(),\n",
    "    'Major Storm Event': nx.Graph(),\n",
    "    'Distribution, Substation, Transmission': nx.Graph()\n",
    "}\n",
    "\n",
    "# Add nodes with replaced spaces\n",
    "for layer in layers:\n",
    "    nodes = [substation.replace(' ', '_') for substation in data['Job Substation'].unique()]\n",
    "    layers[layer].add_nodes_from(nodes)\n",
    "\n",
    "# Define functions to add edges to each layer based on criteria\n",
    "def add_edges_by_column(layer_name, column):\n",
    "    layer = layers[layer_name]\n",
    "    for _, group in data.groupby(column):\n",
    "        nodes = [substation.replace(' ', '_') for substation in group['Job Substation']]\n",
    "        for node1, node2 in itertools.combinations(nodes, 2):\n",
    "            layer.add_edge(node1, node2)\n",
    "\n",
    "def add_edges_by_date(layer_name):\n",
    "    layer = layers[layer_name]\n",
    "    for _, group in data.groupby('Month/Day/Year'):\n",
    "        nodes = [substation.replace(' ', '_') for substation in group['Job Substation']]\n",
    "        for node1, node2 in itertools.combinations(nodes, 2):\n",
    "            layer.add_edge(node1, node2)\n",
    "\n",
    "# Add edges for each layer\n",
    "add_edges_by_column('Job Region', 'Job Region')\n",
    "add_edges_by_column('Job Area (DISTRICT)', 'Job Area (DISTRICT)')\n",
    "add_edges_by_date('Time')\n",
    "add_edges_by_column('Custs Affected Interval', 'Custs Affected Interval')\n",
    "add_edges_by_column('OGE Causes', 'OGE Causes')\n",
    "add_edges_by_column('Major Storm Event', 'Major Storm Event  Y (Yes) or N (No)')\n",
    "add_edges_by_column('Distribution, Substation, Transmission', 'Distribution, Substation, Transmission')\n",
    "\n",
    "# Custom class to represent a multiplex network\n",
    "class MultiplexNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = {}\n",
    "        self.node_set = set()\n",
    "        \n",
    "    def add_layer(self, layer_name, graph):\n",
    "        self.layers[layer_name] = graph\n",
    "        self.node_set.update(graph.nodes)\n",
    "        \n",
    "    def get_layer(self, layer_name):\n",
    "        return self.layers.get(layer_name, None)\n",
    "    \n",
    "    def nodes(self):\n",
    "        return self.node_set\n",
    "    \n",
    "    def edges(self, layer_name=None):\n",
    "        if layer_name:\n",
    "            return self.layers[layer_name].edges\n",
    "        else:\n",
    "            all_edges = {}\n",
    "            for layer, graph in self.layers.items():\n",
    "                all_edges[layer] = list(graph.edges)\n",
    "            return all_edges\n",
    "\n",
    "# Create the multiplex network\n",
    "multiplex_network = MultiplexNetwork()\n",
    "for layer_name, graph in layers.items():\n",
    "    multiplex_network.add_layer(layer_name, graph)\n",
    "\n",
    "# Interact with the multiplex network\n",
    "#print(f\"All nodes in multiplex network: {multiplex_network.nodes()}\")\n",
    "#for layer_name in layers:\n",
    "    #print(f\"Edges in {layer_name} layer: {multiplex_network.edges(layer_name)}\")\n",
    "\n",
    "# print the number of nodes and edges in each layer\n",
    "for layer_name in layers:\n",
    "    print(f\"Number of nodes in {layer_name} layer: {len(layers[layer_name].nodes)}\")\n",
    "    print(f\"Number of edges in {layer_name} layer: {len(layers[layer_name].edges)}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2:  Save adjacency matrices of each layer as CSV files\n",
    "\n",
    "# Define the directory to save the adjacency matrices\n",
    "output_dir = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Multiplex Network'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to save adjacency matrix of each layer\n",
    "def save_adjacency_matrices(multiplex_network, output_dir):\n",
    "    for layer_name, graph in multiplex_network.layers.items():\n",
    "        # Create adjacency matrix\n",
    "        adjacency_matrix = nx.to_numpy_array(graph)\n",
    "        \n",
    "        # Convert adjacency matrix to DataFrame for CSV export\n",
    "        adjacency_df = pd.DataFrame(adjacency_matrix, index=graph.nodes, columns=graph.nodes)\n",
    "        \n",
    "        # Define file path\n",
    "        file_path = os.path.join(output_dir, f\"{layer_name.replace(' ', '_')}_adjacency_matrix.csv\")\n",
    "        \n",
    "        # Save adjacency matrix as .csv file\n",
    "        adjacency_df.to_csv(file_path)\n",
    "        print(f\"Adjacency matrix for layer '{layer_name}' saved at: {file_path}\")\n",
    "\n",
    "# Assuming `multiplex_network` is your existing multiplex network object\n",
    "save_adjacency_matrices(multiplex_network, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Merging the Embeddings with the target column\n",
    "# File paths\n",
    "incidents_file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'\n",
    "embeddings_file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Multiplex Network/r0.25/mltn2v_results.csv'\n",
    "output_file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Multiplex Network/merged_data.csv'\n",
    "\n",
    "# Read the datasets\n",
    "incidents_data = pd.read_excel(incidents_file_path)\n",
    "embeddings_data = pd.read_csv(embeddings_file_path)\n",
    "\n",
    "# Ensure 'Job Substation' in incidents_data matches the embedding keys\n",
    "incidents_data['Job Substation'] = incidents_data['Job Substation'].str.replace(' ', '_')\n",
    "\n",
    "# Reduce incidents_data to necessary columns\n",
    "reduced_incidents_data = incidents_data[['Job Substation', 'Job Area (DISTRICT)']]\n",
    "\n",
    "# Merge the embeddings data with the reduced incidents data\n",
    "merged_data = pd.merge(reduced_incidents_data, embeddings_data, left_on='Job Substation', right_on=embeddings_data.columns[0], how='inner')\n",
    "\n",
    "# Drop the substation identifier column from embeddings data\n",
    "merged_data = merged_data.drop(columns=[embeddings_data.columns[0]])\n",
    "\n",
    "# Rename columns for embeddings\n",
    "embedding_columns = [f'Embedding_{i}' for i in range(1, merged_data.shape[1] - 1)]\n",
    "merged_data.columns = ['Job Substation', 'Job Area (DISTRICT)'] + embedding_columns\n",
    "\n",
    "# Save the merged data to a CSV file\n",
    "merged_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Merged data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prediction through Network Embeddings\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Path to the embedding dataset\n",
    "embedding_file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Multiplex Network/merged_data.csv'\n",
    "\n",
    "# Load the embedding dataset\n",
    "embedding_data = pd.read_csv(embedding_file_path)\n",
    "\n",
    "# Prepare data for the embedding dataset\n",
    "def prepare_embedding_data(data):\n",
    "    X = data.iloc[:, 2:].values  # All columns except the first two\n",
    "    y = data['Job Area (DISTRICT)'].values\n",
    "    return X, y\n",
    "\n",
    "# Encode labels\n",
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded, le\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "def perform_cv(X, y, cv=10):\n",
    "    clf = RandomForestClassifier()\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Process the embedding dataset\n",
    "X, y = prepare_embedding_data(embedding_data)\n",
    "y_encoded, le = encode_labels(y)\n",
    "\n",
    "# Handle missing values (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Perform cross-validation\n",
    "mean_acc, std_acc = perform_cv(X, y_encoded, cv=10)\n",
    "\n",
    "# Print the results\n",
    "print(f'Multiplex Network: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Prediction using the classical machine learning (selected features raw dataset)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Preprocess data: replace spaces in 'Job Substation' names with underscores\n",
    "data['Job Substation'] = data['Job Substation'].str.replace(' ', '_')\n",
    "\n",
    "# Define intervals for 'Custs Affected Interval'\n",
    "custs_intervals = {\n",
    "    'Very Low': (0, 50),\n",
    "    'Low': (51, 100),\n",
    "    'Medium': (101, 500),\n",
    "    'High': (501, float('inf'))\n",
    "}\n",
    "\n",
    "def categorize_custs_affected(affected):\n",
    "    for category, (low, high) in custs_intervals.items():\n",
    "        if low <= affected <= high:\n",
    "            return category\n",
    "    return 'Unknown'\n",
    "\n",
    "# Add a column for categorized customer affected intervals\n",
    "data['Custs Affected Interval'] = data['Custs Affected'].apply(categorize_custs_affected)\n",
    "\n",
    "# Select relevant columns for prediction\n",
    "columns_to_use = [\n",
    "    'Job Region', 'Custs Affected Interval', 'OGE Causes',\n",
    "    'Major Storm Event  Y (Yes) or N (No)', 'Distribution, Substation, Transmission', 'Month/Day/Year'\n",
    "]\n",
    "\n",
    "# Ensure all selected columns are present\n",
    "data = data[columns_to_use + ['Job Area (DISTRICT)']]\n",
    "\n",
    "# Prepare data for the embedding dataset\n",
    "def prepare_embedding_data(data):\n",
    "    X = data.iloc[:, :-1]  # All columns except the last one\n",
    "    y = data['Job Area (DISTRICT)']\n",
    "    return X, y\n",
    "\n",
    "# Encode labels\n",
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded, le\n",
    "\n",
    "# Process the embedding dataset\n",
    "X, y = prepare_embedding_data(data)\n",
    "y_encoded, le = encode_labels(y)\n",
    "\n",
    "# Define preprocessing for numeric and categorical features\n",
    "numeric_features = ['Month/Day/Year']\n",
    "categorical_features = [\n",
    "    'Job Region', 'Custs Affected Interval', 'OGE Causes',\n",
    "    'Major Storm Event  Y (Yes) or N (No)', 'Distribution, Substation, Transmission'\n",
    "]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the model pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "def perform_cv(X, y, cv=10):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Perform cross-validation\n",
    "mean_acc, std_acc = perform_cv(X, y_encoded, cv=10)\n",
    "\n",
    "# Print the results\n",
    "print(f'Multiplex Network: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
