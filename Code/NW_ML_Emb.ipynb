{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Libraries and Raw Dataset\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 important features for predicting 'Job Area (DISTRICT)':\n",
      "                                         Feature  Importance\n",
      "24                           Total Area Premises    0.196319\n",
      "23                         Total Region Premises    0.135345\n",
      "3                                 Job Substation    0.132535\n",
      "5                                      Feeder ID    0.120624\n",
      "2                                     Job Region    0.106200\n",
      "16                                      Job City    0.050573\n",
      "25                          Total Subst Premises    0.049651\n",
      "4                                     Job Feeder    0.038935\n",
      "45  Ark Grid Mod or OK Grid Enhancement Circuits    0.031521\n",
      "26                         Total Feeder Premises    0.022500\n",
      "Top 10 important features for predicting 'Extent':\n",
      "                     Feature  Importance\n",
      "35               Dev Subtype    0.083898\n",
      "29                 Job SAIFI    0.082758\n",
      "10            Custs Affected    0.078632\n",
      "19              CMI Category    0.060542\n",
      "15                  Equip ID    0.058453\n",
      "34               Device Type    0.041994\n",
      "33  STRCTUR_NO/Job Device ID    0.041483\n",
      "16                Equip Desc    0.039945\n",
      "28                 Job SAIDI    0.039600\n",
      "12                       CMI    0.037961\n"
     ]
    }
   ],
   "source": [
    "############# Final Feature Importance for the Target Columns #############\n",
    "# Loading the Raw Dataset\n",
    "file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Function to compute feature importances with stability\n",
    "def compute_feature_importances(data, target_column, n_runs=10):\n",
    "    all_importances = []\n",
    "    feature_names = None\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        # Split the data into features and target\n",
    "        X = data.drop(columns=[target_column])\n",
    "        y = data[target_column]\n",
    "\n",
    "        # Handle categorical variables using one-hot encoding\n",
    "        X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "        # Capture feature names before transforming X\n",
    "        feature_names = X.columns\n",
    "\n",
    "        # Ensure no NaN or infinite values in the dataset\n",
    "        X = np.nan_to_num(X)\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train a RandomForestClassifier\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Capture feature importances\n",
    "        importances = model.feature_importances_\n",
    "        all_importances.append(importances)\n",
    "    \n",
    "    # Average the importances across all runs\n",
    "    avg_importances = np.mean(all_importances, axis=0)\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': avg_importances}).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return importances_df\n",
    "\n",
    "# Convert all object columns to strings to ensure uniformity\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = data[column].astype(str)\n",
    "\n",
    "# Convert datetime columns to numeric timestamps\n",
    "datetime_columns = ['Job OFF Time', 'Job ON Time', 'Month/Day/Year']\n",
    "for column in datetime_columns:\n",
    "    data[column] = pd.to_datetime(data[column], errors='coerce')\n",
    "    data[column] = data[column].astype('int64') // 10**9  # Convert to seconds since epoch\n",
    "\n",
    "# Encode categorical columns using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Identify and remove columns with all NaN values\n",
    "nan_columns = data.columns[data.isna().all()].tolist()\n",
    "data.drop(columns=nan_columns, inplace=True)\n",
    "\n",
    "# Fill missing values with a placeholder first\n",
    "data = data.fillna(-9999)\n",
    "\n",
    "# Impute missing values by replacing the placeholder with the median value of the column\n",
    "imputer = SimpleImputer(strategy='median', missing_values=-9999)\n",
    "data_imputed = imputer.fit_transform(data)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "data = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "\n",
    "# Compute feature importances for 'Job Area (DISTRICT)'\n",
    "important_features_job_area = compute_feature_importances(data, 'Job Area (DISTRICT)', n_runs=10)\n",
    "print(\"Top 10 important features for predicting 'Job Area (DISTRICT)':\")\n",
    "print(important_features_job_area.head(10))\n",
    "\n",
    "# Compute feature importances for 'Extent'\n",
    "important_features_extent = compute_feature_importances(data, 'Extent', n_runs=10)\n",
    "print(\"Top 10 important features for predicting 'Extent':\")\n",
    "print(important_features_extent.head(10))\n",
    "\n",
    "# Display full importance scores for both targets\n",
    "#important_features_job_area, important_features_extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved adjacency matrix for Total Area Premises in network with top 2 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/Adjacency_Matrix_Total Area Premises.csv\n",
      "Saved adjacency matrix for Total Region Premises in network with top 2 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/Adjacency_Matrix_Total Region Premises.csv\n",
      "Saved adjacency matrix for Total Area Premises in network with top 3 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_3_Features/Adjacency_Matrix_Total Area Premises.csv\n",
      "Saved adjacency matrix for Total Region Premises in network with top 3 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_3_Features/Adjacency_Matrix_Total Region Premises.csv\n",
      "Saved adjacency matrix for Job Substation in network with top 3 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_3_Features/Adjacency_Matrix_Job Substation.csv\n",
      "Saved adjacency matrix for Total Area Premises in network with top 4 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_4_Features/Adjacency_Matrix_Total Area Premises.csv\n",
      "Saved adjacency matrix for Total Region Premises in network with top 4 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_4_Features/Adjacency_Matrix_Total Region Premises.csv\n",
      "Saved adjacency matrix for Job Substation in network with top 4 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_4_Features/Adjacency_Matrix_Job Substation.csv\n",
      "Saved adjacency matrix for Feeder ID in network with top 4 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_4_Features/Adjacency_Matrix_Feeder ID.csv\n",
      "Saved adjacency matrix for Total Area Premises in network with top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_5_Features/Adjacency_Matrix_Total Area Premises.csv\n",
      "Saved adjacency matrix for Total Region Premises in network with top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_5_Features/Adjacency_Matrix_Total Region Premises.csv\n",
      "Saved adjacency matrix for Job Substation in network with top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_5_Features/Adjacency_Matrix_Job Substation.csv\n",
      "Saved adjacency matrix for Feeder ID in network with top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_5_Features/Adjacency_Matrix_Feeder ID.csv\n",
      "Saved adjacency matrix for Job Region in network with top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_5_Features/Adjacency_Matrix_Job Region.csv\n",
      "Saved adjacency matrix for Total Area Premises in network with top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_6_Features/Adjacency_Matrix_Total Area Premises.csv\n",
      "Saved adjacency matrix for Total Region Premises in network with top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_6_Features/Adjacency_Matrix_Total Region Premises.csv\n",
      "Saved adjacency matrix for Job Substation in network with top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_6_Features/Adjacency_Matrix_Job Substation.csv\n",
      "Saved adjacency matrix for Feeder ID in network with top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_6_Features/Adjacency_Matrix_Feeder ID.csv\n",
      "Saved adjacency matrix for Job Region in network with top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_6_Features/Adjacency_Matrix_Job Region.csv\n",
      "Saved adjacency matrix for Job City in network with top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_6_Features/Adjacency_Matrix_Job City.csv\n",
      "Saved adjacency matrix for Total Area Premises in network with top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/Adjacency_Matrix_Total Area Premises.csv\n",
      "Saved adjacency matrix for Total Region Premises in network with top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/Adjacency_Matrix_Total Region Premises.csv\n",
      "Saved adjacency matrix for Job Substation in network with top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/Adjacency_Matrix_Job Substation.csv\n",
      "Saved adjacency matrix for Feeder ID in network with top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/Adjacency_Matrix_Feeder ID.csv\n",
      "Saved adjacency matrix for Job Region in network with top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/Adjacency_Matrix_Job Region.csv\n",
      "Saved adjacency matrix for Job City in network with top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/Adjacency_Matrix_Job City.csv\n",
      "Saved adjacency matrix for Total Subst Premises in network with top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/Adjacency_Matrix_Total Subst Premises.csv\n",
      "Finished saving all adjacency matrices.\n"
     ]
    }
   ],
   "source": [
    "############ Constructing Multiplex Networks with Top Features for Job Area (District) ############\n",
    "file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "#nodes \n",
    "unique_substations = data['Job Substation'].unique()\n",
    "\n",
    "# Function to add edges to a graph based on a column\n",
    "def add_edges_by_column(graph, column):\n",
    "    for _, group in data.groupby(column):\n",
    "        nodes = [str(substation).replace(' ', '_') for substation in group['Job Substation']]\n",
    "        for node1, node2 in itertools.combinations(nodes, 2):\n",
    "            graph.add_edge(node1, node2)\n",
    "\n",
    "# Function to construct adjacency matrix for each layer in the multiplex network\n",
    "def get_adjacency_matrix(graph):\n",
    "    return nx.adjacency_matrix(graph, nodelist=unique_substations).todense()\n",
    "\n",
    "# Define the directory to save adjacency matrices\n",
    "output_dir = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Feature importances (mock data)\n",
    "#top_features = ['Total Area Premises', 'Total Region Premises', 'Job Substation', 'Feeder ID', 'Job Region', 'Job City', 'Total Subst Premises']\n",
    "top_features = important_features_job_area.head(7)['Feature'].tolist()\n",
    "\n",
    "\n",
    "# Construct multiplex networks and save individual adjacency matrices for each layer\n",
    "for i in range(2, 8):\n",
    "    features_subset = top_features[:i]\n",
    "    network_folder = os.path.join(output_dir, f'Network_{i}_Features')\n",
    "    os.makedirs(network_folder, exist_ok=True)\n",
    "    \n",
    "    for feature in features_subset:\n",
    "        graph = nx.Graph()\n",
    "        graph.add_nodes_from(unique_substations)  # Ensure all nodes are added first\n",
    "        add_edges_by_column(graph, feature)\n",
    "        \n",
    "        adj_matrix = get_adjacency_matrix(graph)\n",
    "        \n",
    "        # Convert to DataFrame and include node names\n",
    "        adj_df = pd.DataFrame(adj_matrix, index=unique_substations, columns=unique_substations)\n",
    "        \n",
    "        # Save adjacency matrix to CSV file\n",
    "        file_path = os.path.join(network_folder, f'Adjacency_Matrix_{feature}.csv')\n",
    "        adj_df.to_csv(file_path)\n",
    "\n",
    "        # Print confirmation\n",
    "        print(f\"Saved adjacency matrix for {feature} in network with top {i} features to {file_path}\")\n",
    "\n",
    "print(\"Finished saving all adjacency matrices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25/merged_data_with_target.csv\n",
      "    Job Substation Job Area (DISTRICT)  Embedding 1  Embedding 2  Embedding 3  \\\n",
      "0   8617:SUNNYLANE                EAST    -0.166978     0.134816     0.114100   \n",
      "1  5712:JOLLYVILLE             SULPHUR     0.079902     0.078907    -0.314676   \n",
      "2         9410:IGO               OZARK     0.254093    -0.223003    -0.204571   \n",
      "3      7306:FIXICO            SEMINOLE    -0.301220    -0.005030    -0.732102   \n",
      "4     4106:HEMLOCK                ENID    -0.179222     0.041837    -0.324901   \n",
      "\n",
      "   Embedding 4  Embedding 5  Embedding 6  Embedding 7  Embedding 8  ...  \\\n",
      "0    -0.124874    -0.046537     0.165154     0.299822     0.347808  ...   \n",
      "1    -0.091225     0.031908     0.336273     0.206357     0.173952  ...   \n",
      "2     0.233370     0.198864     0.527391     0.548082     0.334865  ...   \n",
      "3     0.312610     0.392512     0.535990     0.886350     0.124509  ...   \n",
      "4     0.098903     0.334397     0.636863     0.588176     0.160303  ...   \n",
      "\n",
      "   Embedding 120  Embedding 121  Embedding 122  Embedding 123  Embedding 124  \\\n",
      "0      -0.186622       0.388533      -0.483564      -0.184165      -0.203157   \n",
      "1       0.242553       0.106699       0.300657      -0.184554       0.044415   \n",
      "2       0.168287      -0.032322      -0.066009       0.229617      -0.355987   \n",
      "3      -0.125652       0.126906       0.336616      -0.181274      -0.125373   \n",
      "4      -0.032940       0.116812       0.108011      -0.058313      -0.130961   \n",
      "\n",
      "   Embedding 125  Embedding 126  Embedding 127  Embedding 128  Embedding 129  \n",
      "0      -0.293741      -0.263989       0.031642      -0.027314      -0.144819  \n",
      "1       0.239500       0.270393      -0.138702       0.225473      -0.299264  \n",
      "2       0.348694       0.522256       0.313538       0.160274       0.156172  \n",
      "3      -0.013270      -0.124205       0.585655      -0.086974       0.095837  \n",
      "4       0.107220       0.148431       0.205351       0.000451       0.031360  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_3_Features/r0.25/merged_data_with_target.csv\n",
      "    Job Substation Job Area (DISTRICT)  Embedding 1  Embedding 2  Embedding 3  \\\n",
      "0   8617:SUNNYLANE                EAST     0.231844     0.170051    -0.360926   \n",
      "1  5712:JOLLYVILLE             SULPHUR     0.346329    -0.152336    -0.131919   \n",
      "2         9410:IGO               OZARK    -0.123270    -0.492901    -1.093687   \n",
      "3      7306:FIXICO            SEMINOLE     0.002580    -0.303519    -0.729069   \n",
      "4     4106:HEMLOCK                ENID    -0.187979     0.157716    -0.678444   \n",
      "\n",
      "   Embedding 4  Embedding 5  Embedding 6  Embedding 7  Embedding 8  ...  \\\n",
      "0    -0.018856    -0.037789     0.321361     0.318206     0.187632  ...   \n",
      "1     0.199436     0.229834     0.098681     0.050167     0.201093  ...   \n",
      "2     0.244858    -0.160922     0.388275    -0.083465    -0.775667  ...   \n",
      "3     0.056510     0.055499     0.283178     0.159174     0.090362  ...   \n",
      "4     0.159942     0.308165     0.323446    -0.050495    -0.312048  ...   \n",
      "\n",
      "   Embedding 120  Embedding 121  Embedding 122  Embedding 123  Embedding 124  \\\n",
      "0      -0.333681      -0.281150       0.263465      -0.042567      -0.075021   \n",
      "1       0.614559      -0.118654       0.299504       0.018812       0.226697   \n",
      "2      -0.381580       0.037119       0.374200      -0.001129       0.149524   \n",
      "3      -0.151295       0.477261       0.714677      -0.550185       0.283567   \n",
      "4       0.253866      -0.456867       0.001748       0.205879       0.617466   \n",
      "\n",
      "   Embedding 125  Embedding 126  Embedding 127  Embedding 128  Embedding 129  \n",
      "0      -0.148040       0.185260       0.781128      -0.676571      -0.239018  \n",
      "1       0.026755       0.591441       0.098729       0.274608      -0.190736  \n",
      "2       0.061279       0.517318       0.381477       0.304039       0.114051  \n",
      "3       0.148889       0.213843       0.553056       0.112812       0.934302  \n",
      "4      -0.257120       0.551500       0.222416       0.046849       0.068974  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_4_Features/r0.25/merged_data_with_target.csv\n",
      "    Job Substation Job Area (DISTRICT)  Embedding 1  Embedding 2  Embedding 3  \\\n",
      "0   8617:SUNNYLANE                EAST    -0.556581    -0.767583     0.348798   \n",
      "1  5712:JOLLYVILLE             SULPHUR    -0.578184    -0.731848    -0.140162   \n",
      "2         9410:IGO               OZARK    -0.285685    -0.421382     0.030089   \n",
      "3      7306:FIXICO            SEMINOLE    -0.424147     0.004548     0.093411   \n",
      "4     4106:HEMLOCK                ENID    -0.396733    -0.786814     0.117274   \n",
      "\n",
      "   Embedding 4  Embedding 5  Embedding 6  Embedding 7  Embedding 8  ...  \\\n",
      "0     0.854332    -0.280395     0.989935     0.337483    -0.792522  ...   \n",
      "1     0.706927     0.312105     0.553255    -0.054436    -0.209545  ...   \n",
      "2     0.420303    -0.601351     0.439872    -0.210547    -0.537728  ...   \n",
      "3     0.203327    -0.408320     0.221142     0.098538    -0.369691  ...   \n",
      "4     0.384103    -0.010534     1.019637     0.326232    -0.845669  ...   \n",
      "\n",
      "   Embedding 120  Embedding 121  Embedding 122  Embedding 123  Embedding 124  \\\n",
      "0      -0.100528      -0.462084      -0.306752       0.272485      -0.004289   \n",
      "1      -0.327497      -0.309898      -0.016835      -0.123904       0.315715   \n",
      "2      -0.341913      -0.114933       0.203426      -0.155869      -0.356059   \n",
      "3      -0.195790      -0.213836      -0.171198       0.259983       0.286707   \n",
      "4      -0.839199       0.245626      -0.711618       0.339210      -0.106618   \n",
      "\n",
      "   Embedding 125  Embedding 126  Embedding 127  Embedding 128  Embedding 129  \n",
      "0       0.090788      -0.413945       0.529636       0.200815       0.730933  \n",
      "1       0.070321      -0.528392       0.647814      -0.251973       0.478410  \n",
      "2       0.006318      -0.596613      -0.172137      -0.720920       0.258942  \n",
      "3       0.122920      -0.596628       0.300022      -0.077815       0.485314  \n",
      "4      -0.314640      -0.252676       0.496874      -0.359469       0.609682  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_5_Features/r0.25/merged_data_with_target.csv\n",
      "    Job Substation Job Area (DISTRICT)  Embedding 1  Embedding 2  Embedding 3  \\\n",
      "0   8617:SUNNYLANE                EAST    -0.151116     0.020785    -0.303093   \n",
      "1  5712:JOLLYVILLE             SULPHUR    -0.213159     0.664225    -0.047133   \n",
      "2         9410:IGO               OZARK    -0.242422     0.115810    -0.455156   \n",
      "3      7306:FIXICO            SEMINOLE    -0.290622     0.057774    -0.233021   \n",
      "4     4106:HEMLOCK                ENID    -0.214696    -0.103824    -0.547532   \n",
      "\n",
      "   Embedding 4  Embedding 5  Embedding 6  Embedding 7  Embedding 8  ...  \\\n",
      "0     0.389088    -0.047308     0.098708     0.286116     0.010765  ...   \n",
      "1     0.071387    -0.462168     0.274126    -0.069492    -0.288869  ...   \n",
      "2     1.350509    -0.267663     0.100112    -0.030934    -0.031118  ...   \n",
      "3     0.423824     0.120042    -0.006800     0.057688    -0.004023  ...   \n",
      "4    -0.286373     0.249744     0.195421     0.391363    -0.379617  ...   \n",
      "\n",
      "   Embedding 120  Embedding 121  Embedding 122  Embedding 123  Embedding 124  \\\n",
      "0      -0.182216       0.226273       0.394930       0.492648      -0.558228   \n",
      "1      -0.016480       0.256719      -0.097615      -0.019173      -0.201429   \n",
      "2       0.089421       0.556920       0.319911       0.227589      -0.201053   \n",
      "3      -0.287926       0.088355       0.233206       0.322274       0.037779   \n",
      "4      -0.588675       0.630301      -0.273620       1.073908      -0.545989   \n",
      "\n",
      "   Embedding 125  Embedding 126  Embedding 127  Embedding 128  Embedding 129  \n",
      "0      -0.281001      -0.134432      -0.373128      -0.464127       0.228313  \n",
      "1       0.223999       0.093646      -0.183011       0.093839       0.222422  \n",
      "2      -0.510871      -0.049239      -0.045877      -0.057770       0.075734  \n",
      "3      -0.244020      -0.258388       0.033020      -0.219912      -0.089263  \n",
      "4      -1.127664       0.513382       0.246816      -0.651790       0.295406  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_6_Features/r0.25/merged_data_with_target.csv\n",
      "    Job Substation Job Area (DISTRICT)  Embedding 1  Embedding 2  Embedding 3  \\\n",
      "0   8617:SUNNYLANE                EAST    -0.019802     0.129051     0.015747   \n",
      "1  5712:JOLLYVILLE             SULPHUR    -0.309596     0.426115    -0.387400   \n",
      "2         9410:IGO               OZARK    -0.090398     0.172059    -0.039720   \n",
      "3      7306:FIXICO            SEMINOLE    -0.391262     0.468817    -0.491776   \n",
      "4     4106:HEMLOCK                ENID    -0.157841     0.216227     0.279270   \n",
      "\n",
      "   Embedding 4  Embedding 5  Embedding 6  Embedding 7  Embedding 8  ...  \\\n",
      "0     0.032728    -0.019704    -0.085460    -0.169945     0.346041  ...   \n",
      "1    -0.438207    -0.276261     0.290546    -0.253030     0.279345  ...   \n",
      "2    -0.106855     0.072294    -0.093034    -0.075017     0.292164  ...   \n",
      "3    -0.494757    -0.415904     0.506647    -0.517197     0.415215  ...   \n",
      "4    -0.392306     0.224447    -0.232781    -0.090577     0.237367  ...   \n",
      "\n",
      "   Embedding 120  Embedding 121  Embedding 122  Embedding 123  Embedding 124  \\\n",
      "0       0.323131      -0.306319      -0.073012      -0.159716      -0.315863   \n",
      "1       0.528796      -0.293390       0.038006      -0.120286      -0.354645   \n",
      "2       0.262709       0.001247      -0.145684      -0.097253      -0.398776   \n",
      "3       0.403746      -0.391768      -0.204019      -0.372910      -0.507746   \n",
      "4       0.213243      -0.162193       0.512016      -0.439887      -0.383280   \n",
      "\n",
      "   Embedding 125  Embedding 126  Embedding 127  Embedding 128  Embedding 129  \n",
      "0       0.104607       0.000181       0.039948       0.014382      -0.238129  \n",
      "1      -0.020091      -0.280269      -0.721397       0.351050       0.363177  \n",
      "2       0.106137      -0.113909      -0.023602       0.058356      -0.212165  \n",
      "3      -0.229793      -0.245065      -0.577568       0.247476      -0.231761  \n",
      "4       0.046916      -0.107841      -0.028172       0.275520      -0.322821  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/r0.25/merged_data_with_target.csv\n",
      "    Job Substation Job Area (DISTRICT)  Embedding 1  Embedding 2  Embedding 3  \\\n",
      "0   8617:SUNNYLANE                EAST     0.238668    -0.078125     0.138798   \n",
      "1  5712:JOLLYVILLE             SULPHUR     0.586385    -0.304902     0.360835   \n",
      "2         9410:IGO               OZARK     0.215447    -0.088595     0.096049   \n",
      "3      7306:FIXICO            SEMINOLE     0.152137    -0.289020     0.339180   \n",
      "4     4106:HEMLOCK                ENID     0.018808     0.184526     1.404139   \n",
      "\n",
      "   Embedding 4  Embedding 5  Embedding 6  Embedding 7  Embedding 8  ...  \\\n",
      "0     0.245149     0.148699     0.037715     0.592117    -0.446769  ...   \n",
      "1    -0.411988     0.530678    -0.139739     0.328380    -0.120616  ...   \n",
      "2    -0.011424     0.225556     0.021858     0.281416    -0.237811  ...   \n",
      "3    -0.060976     0.381363    -0.057877     0.064514    -0.118508  ...   \n",
      "4    -0.565899     0.530738    -0.029658    -0.201174    -0.305512  ...   \n",
      "\n",
      "   Embedding 120  Embedding 121  Embedding 122  Embedding 123  Embedding 124  \\\n",
      "0      -0.013196       0.070668       0.042231      -0.148088       0.171420   \n",
      "1      -0.423745       0.044853       0.819952      -0.272947       0.417128   \n",
      "2      -0.096661      -0.060104      -0.220878      -0.278641       0.161617   \n",
      "3      -0.215372      -0.043126       0.563084      -0.282577       0.264059   \n",
      "4      -0.602508       0.173892      -0.889918       0.228908       0.508430   \n",
      "\n",
      "   Embedding 125  Embedding 126  Embedding 127  Embedding 128  Embedding 129  \n",
      "0       0.208558       0.200648      -0.041141       0.063627      -0.040215  \n",
      "1      -0.186974      -0.368885       0.532376       0.607799      -0.256053  \n",
      "2       0.082172       0.228720       0.057185       0.076442      -0.126275  \n",
      "3       0.050737      -0.064373       0.312410       0.153166       0.172955  \n",
      "4       0.580567      -0.606294       0.021520       0.937872       0.086965  \n",
      "\n",
      "[5 rows x 131 columns]\n"
     ]
    }
   ],
   "source": [
    "######### Prepare the datasets for prediction of Job Area (District) #########\n",
    "\n",
    "# Preparing the datasets for Machine Learning\n",
    "file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure 'Job Substation' in data matches the embedding keys\n",
    "data['Job Substation'] = data['Job Substation'].astype(str).str.replace(' ', '_')\n",
    "\n",
    "# Merge the embeddings data with the 'Job Area (DISTRICT)' column from the incidents data\n",
    "reduced_incidents_data = data[['Job Substation', 'Job Area (DISTRICT)']]\n",
    "\n",
    "# List of embedding paths\n",
    "embedding_paths = [\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_3_Features/r0.25',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_4_Features/r0.25',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_5_Features/r0.25',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_6_Features/r0.25',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_7_Features/r0.25'\n",
    "]\n",
    "\n",
    "# Iterate over each embedding path\n",
    "for embedding_path in embedding_paths:\n",
    "    nw_feature = os.path.basename(os.path.dirname(embedding_path))\n",
    "    embeddings_file_path = os.path.join(embedding_path, 'mltn2v_results.csv')\n",
    "\n",
    "    # Read the embeddings data\n",
    "    embeddings_data = pd.read_csv(embeddings_file_path)\n",
    "\n",
    "    # Merge the embeddings data with the reduced incidents data\n",
    "    merged_data_corrected = pd.merge(reduced_incidents_data, embeddings_data, left_on='Job Substation', right_on=embeddings_data.columns[0], how='inner')\n",
    "\n",
    "    # Drop the substation identifier column from embeddings data\n",
    "    merged_data_corrected = merged_data_corrected.drop(columns=[embeddings_data.columns[0]])\n",
    "\n",
    "    # Rename columns for embeddings\n",
    "    embedding_columns = [f'Embedding {i+1}' for i in range(merged_data_corrected.shape[1] - 2)]\n",
    "    merged_data_corrected.columns = ['Job Substation', 'Job Area (DISTRICT)'] + embedding_columns\n",
    "\n",
    "    # Save the merged data to a CSV file\n",
    "    output_file_path = os.path.join(embedding_path, 'merged_data_with_target.csv')\n",
    "    merged_data_corrected.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Merged data saved to {output_file_path}\")\n",
    "\n",
    "    # Display the first few rows of the merged data\n",
    "    print(merged_data_corrected.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [1.         1.         0.99328859 1.         1.         1.\n",
      " 0.99326599 1.         1.         1.        ]\n",
      "Mean accuracy: 0.9986554583870021\n"
     ]
    }
   ],
   "source": [
    "############## Testing on 2 Features Network Embeddings ##############\n",
    "\n",
    "# Load the merged data\n",
    "merged_data_file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25/merged_data_with_target.csv' \n",
    "merged_data = pd.read_csv(merged_data_file_path)\n",
    "\n",
    "# Separate features and target\n",
    "X = merged_data.drop(columns=['Job Area (DISTRICT)', 'Job Substation'])\n",
    "y = merged_data['Job Area (DISTRICT)']\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print the accuracy for each fold and the mean accuracy\n",
    "print(f\"Accuracy for each fold: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 important features:\n",
      "         Feature  Importance\n",
      "57  Embedding 58    0.027060\n",
      "49  Embedding 50    0.026653\n",
      "27  Embedding 28    0.026600\n",
      "50  Embedding 51    0.025443\n",
      "29  Embedding 30    0.023977\n",
      "46  Embedding 47    0.022677\n",
      "59  Embedding 60    0.021852\n",
      "19  Embedding 20    0.019102\n",
      "41  Embedding 42    0.017377\n",
      "51  Embedding 52    0.016373\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### Feature Importance for 2 Features Network Embeddings ########\n",
    "\n",
    "# Train the model on the full dataset to get feature importances\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top 10 important features\n",
    "print(\"Top 10 important features:\")\n",
    "print(feature_importances_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged_2F_NW_data.csv: Mean Accuracy = 0.9993, Standard Deviation = 0.0013\n",
      "Merged_3F_NW_data.csv: Mean Accuracy = 0.9993, Standard Deviation = 0.0013\n",
      "Merged_4F_NW_data.csv: Mean Accuracy = 0.9993, Standard Deviation = 0.0013\n",
      "Merged_5F_NW_data.csv: Mean Accuracy = 0.9993, Standard Deviation = 0.0013\n",
      "Merged_6F_NW_data.csv: Mean Accuracy = 0.9993, Standard Deviation = 0.0013\n",
      "Merged_7F_NW_data.csv: Mean Accuracy = 0.9993, Standard Deviation = 0.0013\n",
      "Incidents_5000.xlsx: Mean Accuracy = 0.9987, Standard Deviation = 0.0027\n"
     ]
    }
   ],
   "source": [
    "############# Final Prediction of Job Area (District) ############\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the datasets\n",
    "embedding_paths = [\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25/merged_data_with_target.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25/merged_data_with_target.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25/merged_data_with_target.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25/merged_data_with_target.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25/merged_data_with_target.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/adjacency_matrices/Network_2_Features/r0.25/merged_data_with_target.csv'\n",
    "]\n",
    "\n",
    "# Load embedding datasets\n",
    "embedding_data = [pd.read_csv(path) for path in embedding_paths]\n",
    "\n",
    "# Prepare data for embedding datasets\n",
    "def prepare_embedding_data(data):\n",
    "    X = data.iloc[:, 2:].values  # All columns except the first two\n",
    "    y = data['Job Area (DISTRICT)'].values\n",
    "    return X, y\n",
    "\n",
    "# Encode labels\n",
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded, le\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "def perform_cv(X, y, cv=10):\n",
    "    clf = RandomForestClassifier()\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "# Process embedding datasets\n",
    "for i, data in enumerate(embedding_data, start=2):\n",
    "    X, y = prepare_embedding_data(data)\n",
    "    y_encoded, _ = encode_labels(y)\n",
    "    mean_acc, std_acc = perform_cv(X, y_encoded, cv=10)\n",
    "    results[f'Merged_{i}F_NW_data.csv'] = (mean_acc, std_acc)\n",
    "\n",
    "# Prepare raw dataset\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category', 'datetime']).columns.tolist()\n",
    "numerical_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols.remove('Job Area (DISTRICT)')\n",
    "\n",
    "X_raw = data.drop(columns=['Job Area (DISTRICT)'])\n",
    "y_raw = data['Job Area (DISTRICT)']\n",
    "\n",
    "# Convert all categorical columns to string\n",
    "X_raw[categorical_cols] = X_raw[categorical_cols].astype(str)\n",
    "\n",
    "# Define preprocessing for all columns\n",
    "numerical_transformer_all = SimpleImputer(strategy='mean')\n",
    "categorical_transformer_all = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for all numerical and categorical data\n",
    "preprocessor_all = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer_all, numerical_cols),\n",
    "        ('cat', categorical_transformer_all, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create the pipeline with all features\n",
    "clf_pipeline_all = Pipeline(steps=[('preprocessor', preprocessor_all),\n",
    "                                   ('model', RandomForestClassifier())])\n",
    "\n",
    "# Encode labels for the raw dataset\n",
    "y_raw_encoded, _ = encode_labels(y_raw)\n",
    "\n",
    "# Perform cross-validation on the raw dataset\n",
    "scores_raw = cross_val_score(clf_pipeline_all, X_raw, y_raw_encoded, cv=10, scoring='accuracy')\n",
    "results['Incidents_5000.xlsx'] = (scores_raw.mean(), scores_raw.std())\n",
    "\n",
    "# Print the results\n",
    "for dataset, (mean_acc, std_acc) in results.items():\n",
    "    print(f'{dataset}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Feature  Importance\n",
      "35               Dev Subtype    0.083898\n",
      "29                 Job SAIFI    0.082758\n",
      "10            Custs Affected    0.078632\n",
      "19              CMI Category    0.060542\n",
      "15                  Equip ID    0.058453\n",
      "34               Device Type    0.041994\n",
      "33  STRCTUR_NO/Job Device ID    0.041483\n",
      "16                Equip Desc    0.039945\n",
      "28                 Job SAIDI    0.039600\n",
      "12                       CMI    0.037961\n"
     ]
    }
   ],
   "source": [
    "# print the extent top feature \n",
    "\n",
    "print(important_features_extent.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Job Display ID             CAD_ID  Job Region Job Area (DISTRICT)  \\\n",
      "0   J2001.000006  PD-01012020-00063  METRO EAST                EAST   \n",
      "1   J2001.000021  PD-01012020-00363   NORTHWEST            WOODWARD   \n",
      "2   J2001.000041                NaN  METRO WEST             EL RENO   \n",
      "3   J2001.000045  PD-01012020-00859    SOUTHERN             SULPHUR   \n",
      "4   J2001.000046  PD-01012020-00858   NORTHWEST            WOODWARD   \n",
      "\n",
      "    Job Substation       Job Feeder  Feeder ID        Job OFF Time  \\\n",
      "0   8617:SUNNYLANE   SUNNYLANE_1722     861722 2020-01-01 00:21:50   \n",
      "1   4606:CEDAR AVE    CEDAR_AVE_631     460631 2020-01-01 03:00:30   \n",
      "2     8905:EL RENO      EL_RENO_522     890522 2020-01-01 08:39:50   \n",
      "3  5712:JOLLYVILLE  JOLLYVILLE_1264     571264 2020-01-01 08:48:18   \n",
      "4   4606:CEDAR AVE    CEDAR_AVE_622     460622 2020-01-01 08:50:25   \n",
      "\n",
      "          Job ON Time  Job Duration Mins  ...  Feeder SAIDI  AM Notes  \\\n",
      "0 2020-01-01 09:22:20             540.50  ...           NaN       NaN   \n",
      "1 2020-01-01 04:23:00              82.50  ...           NaN       NaN   \n",
      "2 2020-01-01 09:56:25              76.58  ...           NaN       NaN   \n",
      "3 2020-01-01 13:39:00             290.70  ...           NaN       NaN   \n",
      "4 2020-01-01 11:00:00             129.58  ...           NaN       NaN   \n",
      "\n",
      "        OGE Causes  Major Storm Event  Y (Yes) or N (No)  \\\n",
      "0  Cause Exclusion                                     N   \n",
      "1        Equipment                                     N   \n",
      "2        Equipment                                     N   \n",
      "3        Equipment                                     N   \n",
      "4        Equipment                                     N   \n",
      "\n",
      "  Distribution, Substation, Transmission  \\\n",
      "0                           DISTRIBUTION   \n",
      "1                           DISTRIBUTION   \n",
      "2                           DISTRIBUTION   \n",
      "3                           DISTRIBUTION   \n",
      "4                           DISTRIBUTION   \n",
      "\n",
      "   Transmission Voltage (69kV, 138kV, 161kv) feeding distribution substation  \\\n",
      "0                                               69kV                           \n",
      "1                                               69kV                           \n",
      "2                                              138kV                           \n",
      "3                                              138kV                           \n",
      "4                                               69kV                           \n",
      "\n",
      "  Month/Day/Year  Year  \\\n",
      "0     2020-01-01  2020   \n",
      "1     2020-01-01  2020   \n",
      "2     2020-01-01  2020   \n",
      "3     2020-01-01  2020   \n",
      "4     2020-01-01  2020   \n",
      "\n",
      "  Equipment Desc that should be excluded from reported indices  \\\n",
      "0                                                NaN             \n",
      "1                                                NaN             \n",
      "2                                                NaN             \n",
      "3                                                NaN             \n",
      "4                                                NaN             \n",
      "\n",
      "  Ark Grid Mod or OK Grid Enhancement Circuits  \n",
      "0                                          NaN  \n",
      "1                                          NaN  \n",
      "2                                          NaN  \n",
      "3                                          NaN  \n",
      "4                                          NaN  \n",
      "\n",
      "[5 rows x 52 columns]\n",
      "Saved adjacency matrix for layer Dev Subtype of top 2 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_2_Features/Adjacency_Matrix_Layer_Dev_Subtype.csv\n",
      "Saved adjacency matrix for layer Job SAIFI of top 2 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_2_Features/Adjacency_Matrix_Layer_Job_SAIFI.csv\n",
      "Saved adjacency matrix for layer Dev Subtype of top 3 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_3_Features/Adjacency_Matrix_Layer_Dev_Subtype.csv\n",
      "Saved adjacency matrix for layer Job SAIFI of top 3 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_3_Features/Adjacency_Matrix_Layer_Job_SAIFI.csv\n",
      "Saved adjacency matrix for layer Custs Affected of top 3 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_3_Features/Adjacency_Matrix_Layer_Custs_Affected.csv\n",
      "Saved adjacency matrix for layer Dev Subtype of top 4 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_4_Features/Adjacency_Matrix_Layer_Dev_Subtype.csv\n",
      "Saved adjacency matrix for layer Job SAIFI of top 4 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_4_Features/Adjacency_Matrix_Layer_Job_SAIFI.csv\n",
      "Saved adjacency matrix for layer Custs Affected of top 4 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_4_Features/Adjacency_Matrix_Layer_Custs_Affected.csv\n",
      "Saved adjacency matrix for layer CMI Category of top 4 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_4_Features/Adjacency_Matrix_Layer_CMI_Category.csv\n",
      "Saved adjacency matrix for layer Dev Subtype of top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_5_Features/Adjacency_Matrix_Layer_Dev_Subtype.csv\n",
      "Saved adjacency matrix for layer Job SAIFI of top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_5_Features/Adjacency_Matrix_Layer_Job_SAIFI.csv\n",
      "Saved adjacency matrix for layer Custs Affected of top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_5_Features/Adjacency_Matrix_Layer_Custs_Affected.csv\n",
      "Saved adjacency matrix for layer CMI Category of top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_5_Features/Adjacency_Matrix_Layer_CMI_Category.csv\n",
      "Saved adjacency matrix for layer Equip ID of top 5 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_5_Features/Adjacency_Matrix_Layer_Equip_ID.csv\n",
      "Saved adjacency matrix for layer Dev Subtype of top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/Adjacency_Matrix_Layer_Dev_Subtype.csv\n",
      "Saved adjacency matrix for layer Job SAIFI of top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/Adjacency_Matrix_Layer_Job_SAIFI.csv\n",
      "Saved adjacency matrix for layer Custs Affected of top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/Adjacency_Matrix_Layer_Custs_Affected.csv\n",
      "Saved adjacency matrix for layer CMI Category of top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/Adjacency_Matrix_Layer_CMI_Category.csv\n",
      "Saved adjacency matrix for layer Equip ID of top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/Adjacency_Matrix_Layer_Equip_ID.csv\n",
      "Saved adjacency matrix for layer Device Type of top 6 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/Adjacency_Matrix_Layer_Device_Type.csv\n",
      "Saved adjacency matrix for layer Dev Subtype of top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/Adjacency_Matrix_Layer_Dev_Subtype.csv\n",
      "Saved adjacency matrix for layer Job SAIFI of top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/Adjacency_Matrix_Layer_Job_SAIFI.csv\n",
      "Saved adjacency matrix for layer Custs Affected of top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/Adjacency_Matrix_Layer_Custs_Affected.csv\n",
      "Saved adjacency matrix for layer CMI Category of top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/Adjacency_Matrix_Layer_CMI_Category.csv\n",
      "Saved adjacency matrix for layer Equip ID of top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/Adjacency_Matrix_Layer_Equip_ID.csv\n",
      "Saved adjacency matrix for layer Device Type of top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/Adjacency_Matrix_Layer_Device_Type.csv\n",
      "Saved adjacency matrix for layer STRCTUR_NO/Job Device ID of top 7 features to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/Adjacency_Matrix_Layer_STRCTUR_NO_Job_Device_ID.csv\n",
      "Combined Adjacency Matrices have been saved.\n"
     ]
    }
   ],
   "source": [
    "############### Constructing Multiplex Networks for Top Features of Predicting Extent ###############\n",
    "\n",
    "# Define the MultiplexNetwork class\n",
    "class MultiplexNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = {}\n",
    "        self.node_set = set()\n",
    "        \n",
    "    def add_layer(self, layer_name, graph):\n",
    "        self.layers[layer_name] = graph\n",
    "        self.node_set.update(graph.nodes)\n",
    "        \n",
    "    def get_layer(self, layer_name):\n",
    "        return self.layers.get(layer_name, None)\n",
    "    \n",
    "    def nodes(self):\n",
    "        return self.node_set\n",
    "    \n",
    "    def edges(self, layer_name=None):\n",
    "        if layer_name:\n",
    "            return self.layers[layer_name].edges\n",
    "        else:\n",
    "            all_edges = {}\n",
    "            for layer, graph in self.layers.items():\n",
    "                all_edges[layer] = list(graph.edges)\n",
    "            return all_edges\n",
    "\n",
    "# Ensure we have the correct raw dataset\n",
    "file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Print the first few rows to confirm data is loaded correctly\n",
    "print(data.head())\n",
    "\n",
    "# The top 7 features for predicting Extent\n",
    "top_7_features = important_features_extent.head(7)['Feature'].tolist()\n",
    "\n",
    "# Verify if the features exist in the dataset\n",
    "missing_features = [feature for feature in top_7_features if feature not in data.columns]\n",
    "if missing_features:\n",
    "    raise KeyError(f\"The following important features are missing from the dataset: {missing_features}\")\n",
    "\n",
    "# Get the unique 'Job Substation' values to be used as nodes in all layers\n",
    "unique_substations = [str(substation).replace(' ', '_') for substation in data['Job Substation'].unique()]\n",
    "\n",
    "# Function to add edges to a graph based on a column\n",
    "def add_edges_by_column(graph, column):\n",
    "    for _, group in data.groupby(column):\n",
    "        nodes = [str(substation).replace(' ', '_') for substation in group['Job Substation']]\n",
    "        for node1, node2 in itertools.combinations(nodes, 2):\n",
    "            graph.add_edge(node1, node2)\n",
    "\n",
    "# Function to construct adjacency matrix for each layer in the multiplex network\n",
    "def get_adjacency_matrix(graph):\n",
    "    return nx.adjacency_matrix(graph, nodelist=unique_substations).todense()\n",
    "\n",
    "# Directory to save adjacency matrices\n",
    "output_base_dir = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices'\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "# Construct multiplex networks and save adjacency matrices\n",
    "for i in range(2, 8):\n",
    "    features_subset = top_7_features[:i]\n",
    "    \n",
    "    multiplex_network = MultiplexNetwork()\n",
    "    \n",
    "    # Create a directory for this network\n",
    "    network_dir = os.path.join(output_base_dir, f'Network_{i}_Features')\n",
    "    os.makedirs(network_dir, exist_ok=True)\n",
    "    \n",
    "    for feature in features_subset:\n",
    "        graph = nx.Graph()\n",
    "        graph.add_nodes_from(unique_substations)  # Ensure all nodes are added first\n",
    "        add_edges_by_column(graph, feature)\n",
    "        \n",
    "        multiplex_network.add_layer(feature, graph)\n",
    "        \n",
    "        adj_matrix = get_adjacency_matrix(graph)\n",
    "        \n",
    "        # Save adjacency matrix to CSV file\n",
    "        safe_feature_name = feature.replace(' ', '_').replace('/', '_')\n",
    "        file_path = os.path.join(network_dir, f'Adjacency_Matrix_Layer_{safe_feature_name}.csv')\n",
    "        adj_df = pd.DataFrame(adj_matrix, index=unique_substations, columns=unique_substations)\n",
    "        adj_df.to_csv(file_path)\n",
    "\n",
    "        # Print confirmation\n",
    "        print(f\"Saved adjacency matrix for layer {feature} of top {i} features to {file_path}\")\n",
    "\n",
    "print(\"Combined Adjacency Matrices have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_2_Features/r0.25/Merged_2F_NW_data.csv\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_3_Features/r0.25/Merged_3F_NW_data.csv\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_4_Features/r0.25/Merged_4F_NW_data.csv\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_5_Features/r0.25/Merged_5F_NW_data.csv\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/r0.25/Merged_6F_NW_data.csv\n",
      "Merged data saved to /Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/r0.25/Merged_7F_NW_data.csv\n",
      "All merged datasets have been saved.\n"
     ]
    }
   ],
   "source": [
    "############ Preparing the NW Embeddings for Machine Learning to Predict Extent ############\n",
    "\n",
    "# File paths\n",
    "incidents_file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'  # Update with your file path\n",
    "\n",
    "# Embeddings paths\n",
    "embeddings_paths = {\n",
    "    2: '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_2_Features/r0.25/mltn2v_results.csv',\n",
    "    3: '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_3_Features/r0.25/mltn2v_results.csv',\n",
    "    4: '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_4_Features/r0.25/mltn2v_results.csv',\n",
    "    5: '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_5_Features/r0.25/mltn2v_results.csv',\n",
    "    6: '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/r0.25/mltn2v_results.csv',\n",
    "    7: '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/r0.25/mltn2v_results.csv',\n",
    "}\n",
    "\n",
    "# Load the incidents data\n",
    "incidents_data = pd.read_excel(incidents_file_path, engine='openpyxl')\n",
    "\n",
    "# Ensure 'Job Substation' in incidents_data matches the embedding keys\n",
    "incidents_data['Job Substation'] = incidents_data['Job Substation'].str.replace(' ', '_')\n",
    "\n",
    "# Function to merge embeddings with incidents data\n",
    "def merge_embeddings_with_incidents(incidents_data, embeddings_path, num_features):\n",
    "    # Read the embeddings data\n",
    "    embeddings_data = pd.read_csv(embeddings_path)\n",
    "\n",
    "    # Merge the embeddings data with the 'Job Substation' and 'Extent' columns from the incidents data\n",
    "    reduced_incidents_data = incidents_data[['Job Substation', 'Extent']]\n",
    "\n",
    "    # Merge the embeddings data with the reduced incidents data\n",
    "    merged_data = pd.merge(reduced_incidents_data, embeddings_data, left_on='Job Substation', right_on=embeddings_data.columns[0], how='inner')\n",
    "\n",
    "    # Drop the substation identifier column from embeddings data\n",
    "    merged_data = merged_data.drop(columns=[embeddings_data.columns[0]])\n",
    "\n",
    "    # Rename columns for embeddings\n",
    "    embedding_columns = [f'Embedding {i+1}' for i in range(merged_data.shape[1] - 2)]\n",
    "    merged_data.columns = ['Job Substation', 'Extent'] + embedding_columns\n",
    "\n",
    "    # Save the merged data to a CSV file in the same folder as the embeddings\n",
    "    output_dir = os.path.dirname(embeddings_path)\n",
    "    output_file_path = os.path.join(output_dir, f'Merged_{num_features}F_NW_data.csv')\n",
    "    merged_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Merged data saved to {output_file_path}\")\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "# Loop through each embeddings path and merge with incidents data\n",
    "for num_features, embeddings_path in embeddings_paths.items():\n",
    "    merge_embeddings_with_incidents(incidents_data, embeddings_path, num_features)\n",
    "\n",
    "print(\"All merged datasets have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged_2F_NW_data.csv: Mean Accuracy = 0.5480, Standard Deviation = 0.0155\n",
      "Merged_3F_NW_data.csv: Mean Accuracy = 0.5464, Standard Deviation = 0.0146\n",
      "Merged_4F_NW_data.csv: Mean Accuracy = 0.5466, Standard Deviation = 0.0169\n",
      "Merged_5F_NW_data.csv: Mean Accuracy = 0.5462, Standard Deviation = 0.0154\n",
      "Merged_6F_NW_data.csv: Mean Accuracy = 0.5478, Standard Deviation = 0.0157\n",
      "Merged_7F_NW_data.csv: Mean Accuracy = 0.5478, Standard Deviation = 0.0166\n",
      "Incidents_5000.xlsx: Mean Accuracy = 0.8226, Standard Deviation = 0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadkazim/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Region SAIDI' 'Area SAIDI' 'Subst SAIDI' 'Feeder SAIDI'\n",
      " 'Equipment Desc that should be excluded from reported indices']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning Model for NW Embeddings \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Paths to the datasets\n",
    "embedding_paths = [\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_2_Features/r0.25/Merged_2F_NW_data.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_3_Features/r0.25/Merged_3F_NW_data.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_4_Features/r0.25/Merged_4F_NW_data.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_5_Features/r0.25/Merged_5F_NW_data.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_6_Features/r0.25/Merged_6F_NW_data.csv',\n",
    "    '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Embeddings_Extent/Networks_Adjacency_Matrices/Network_7_Features/r0.25/Merged_7F_NW_data.csv',\n",
    "]\n",
    "\n",
    "raw_data_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/SPP/Incidents_5000.xlsx'  # Update with your file path\n",
    "\n",
    "# Load datasets\n",
    "embedding_data = [pd.read_csv(path) for path in embedding_paths]\n",
    "raw_data = pd.read_excel(raw_data_path, engine='openpyxl')\n",
    "\n",
    "# Prepare data for embedding datasets\n",
    "def prepare_embedding_data(data):\n",
    "    X = data.iloc[:, 2:].values  # All columns except the first two\n",
    "    y = data['Extent'].values\n",
    "    return X, y\n",
    "\n",
    "# Encode labels\n",
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded, le\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "def perform_cv(X, y, cv=10):\n",
    "    clf = RandomForestClassifier()\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "# Process embedding datasets\n",
    "for i, data in enumerate(embedding_data, start=2):\n",
    "    X, y = prepare_embedding_data(data)\n",
    "    y_encoded, _ = encode_labels(y)\n",
    "    mean_acc, std_acc = perform_cv(X, y_encoded, cv=10)\n",
    "    results[f'Merged_{i}F_NW_data.csv'] = (mean_acc, std_acc)\n",
    "\n",
    "# Prepare raw dataset\n",
    "categorical_cols = raw_data.select_dtypes(include=['object', 'category', 'datetime']).columns.tolist()\n",
    "numerical_cols = raw_data.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols.remove('Extent')\n",
    "\n",
    "X_raw = raw_data.drop(columns=['Extent'])\n",
    "y_raw = raw_data['Extent']\n",
    "\n",
    "# Convert all categorical columns to string\n",
    "X_raw[categorical_cols] = X_raw[categorical_cols].astype(str)\n",
    "\n",
    "# Define preprocessing for all columns\n",
    "numerical_transformer_all = SimpleImputer(strategy='mean')\n",
    "categorical_transformer_all = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for all numerical and categorical data\n",
    "preprocessor_all = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer_all, numerical_cols),\n",
    "        ('cat', categorical_transformer_all, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create the pipeline with all features\n",
    "clf_pipeline_all = Pipeline(steps=[('preprocessor', preprocessor_all),\n",
    "                                   ('model', RandomForestClassifier())])\n",
    "\n",
    "# Encode labels for the raw dataset\n",
    "y_raw_encoded, _ = encode_labels(y_raw)\n",
    "\n",
    "# Perform cross-validation on the raw dataset\n",
    "scores_raw = cross_val_score(clf_pipeline_all, X_raw, y_raw_encoded, cv=10, scoring='accuracy')\n",
    "results['Incidents_5000.xlsx'] = (scores_raw.mean(), scores_raw.std())\n",
    "\n",
    "# Print the results\n",
    "for dataset, (mean_acc, std_acc) in results.items():\n",
    "    print(f'{dataset}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
